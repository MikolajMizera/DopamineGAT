{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation of dopaminergic D3 binding QSAR model to novel scaffolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to investigate the applicability of QSAR to the prediction of the binding affinity of compounds with novel chemical scaffolds. From the ML perspective, it is a problem of extrapolation of a model to out-of-distribution (OOD) observations. This problem is highly relevant to assessing the applicability of QSAR models to the exploration of the novel chemical space.\n",
    "\n",
    "In this notebook:\n",
    "1. Bioactivities and chemical structures from ChemBL are curated\n",
    "2. Compounds are clustered by scaffolds and separated accordingly into groups (distributions)\n",
    "3. Deep GNN model is created\n",
    "4. Model is trained and validated in two data regimes:\n",
    "    - IID regime with train/validation/test observations sampled from the same distribution (same scaffold cluster)\n",
    "    - OOD. regime with train/validation/test observations sampled from random distribution (different scaffold cluster)\n",
    "5. The distributionally robust optimization is used to improve model performance in OOD regime\n",
    "6. The results are compared and concluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rdkit.Chem.rdmolops import RemoveHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import ChemBLDataset\n",
    "from utils.clustering import cluster_by_scaffold, plot_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_sdf = 'D3_ChemBL.sdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj_mizera/anaconda3/envs/env_test/lib/python3.8/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eedba38c3844e687315149808cf64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=313), Label(value='0 / 313'))), HBâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    cache = pd.read_pickle('cache_%s.pkl'%standardized_sdf)\n",
    "    bioactivities, observation_counts, descriptors_df = cache\n",
    "except:\n",
    "    data_curator = ChemBLDataset(standardized_sdf)\n",
    "    bioactivities, observation_counts, descriptors_df = data_curator.get_curated(\n",
    "        assays_thershold=5,\n",
    "        mol_hash='descriptor')\n",
    "    pd.to_pickle((bioactivities, observation_counts, descriptors_df), \n",
    "                 'cache_%s.pkl'%standardized_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipeline = make_pipeline(StandardScaler(), PCA(32))\n",
    "descs_pca = pca_pipeline.fit_transform(descriptors_df)\n",
    "descs_pca = pd.DataFrame(index=descriptors_df.index, data=descs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and features space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compounds are clustered by scaffolds with ScaffoldTree algorithm. The largest 10 clusters are kept and considered as clusters of compounds sampled from separate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels, full_labels = cluster_by_scaffold(bioactivities, descriptors_df, n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scaffold_cls = pd.DataFrame(full_labels).value_counts().head(10).index\n",
    "top_scaffold_cls = np.concatenate(top_scaffold_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioactivities['cluster'] = full_labels\n",
    "bioactivities = bioactivities[bioactivities['cluster'].isin(top_scaffold_cls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bioactivities['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_df = descriptors_df.loc[bioactivities['mol_hash']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compounds are visualized in TSNE-reduced descriptor space to visualize the similarity of different clusters (colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tsne(bioactivities, descriptors_df, bioactivities['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target (pKi affinity) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('talk', font_scale=1.5)\n",
    "sns.histplot(bioactivities['mean'], discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with molecular graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets.molecule_net import x_map, e_map\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_mappings = {\n",
    "    'atomic_num': lambda x: x.GetAtomicNum(),\n",
    "    'chirality': lambda x: str(x.GetChiralTag()),\n",
    "    'degree': lambda x: x.GetTotalDegree(),\n",
    "    'formal_charge': lambda x: x.GetFormalCharge(),\n",
    "    'num_hs': lambda x: x.GetTotalNumHs(),\n",
    "    'num_radical_electrons': lambda x: x.GetNumRadicalElectrons(),\n",
    "    'hybridization': lambda x: str(x.GetHybridization()),\n",
    "    'is_aromatic': lambda x: x.GetIsAromatic(),\n",
    "    'is_in_ring': lambda x: x.IsInRing(),\n",
    "    'bond_type': lambda x: str(x.GetBondType()),\n",
    "    'stereo': lambda x: str(x.GetStereo()),\n",
    "    'is_conjugated': lambda x: x.GetIsConjugated()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted form torch_geometric.datasets import MoleculeNet\n",
    "\n",
    "def mol_to_graph(x):\n",
    "        \n",
    "    mol = x.loc['mols']\n",
    "#     mol = RemoveHs(mol, implicitOnly=True, sanitize=False)\n",
    "    y = x.loc['mean']\n",
    "    mol_hash = x.loc['mol_hash']\n",
    "    cluster = x.loc['cluster']\n",
    "    \n",
    "    desc_vec = descs_pca.loc[mol_hash].values[None, :]\n",
    "    desc_vec = torch.tensor(desc_vec)\n",
    "    \n",
    "    # map atom properties to node features\n",
    "    node_features = [[x_map[k].index(att(atom)) \n",
    "                      for k, att in prop_mappings.items()\n",
    "                         if k in x_map.keys()]\n",
    "                             for atom in mol.GetAtoms()]\n",
    "    \n",
    "    x = torch.tensor(node_features, dtype=torch.long).view(-1, 9)\n",
    "\n",
    "    # map bond properties to edge features\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        \n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = [e_map[k].index(att(bond)) \n",
    "             for k, att in prop_mappings.items()\n",
    "                if k in e_map.keys()]\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    if edge_index.numel() > 0:\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, \n",
    "                desc=desc_vec, cluster=cluster)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = bioactivities[['mol_hash', 'mols', 'mean', 'cluster']].apply(mol_to_graph, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = dataset[0].x.shape[1]\n",
    "num_edge_features = int(dataset[0].edge_attr.shape[-1])\n",
    "descs_comp = dataset[0].desc.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-based neural model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GNN will quantitatively model the relationship between chemical structure and binding affinity to Dopamine receptor. \n",
    "\n",
    "The main design assumptions are: \n",
    " - use 3 convolutional layers with an attention mechanism to aggregate features from 3rd neighbour of each node (atom) in a graph\n",
    " - use normalization of graph nodes to prevent over-smoothing\n",
    " - get graph-level embeddings with a global pooling operator\n",
    " - predict binding affinity with graph embeddings as features\n",
    " - use MC dropout to model uncertainty of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, BatchNorm1d, Dropout, ReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GATv2Conv, TopKPooling, PairNorm\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DopamineGAT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_edge_features, \n",
    "                 gat_hidden_size=8, \n",
    "                 edge_nn_hidden_size=8,\n",
    "                 dense_hidden_size=32,\n",
    "                 n_gat_heads=3):\n",
    "        \n",
    "        super(DopamineGAT, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        ### Graph embedding block\n",
    "        self.conv_1 = GATv2Conv(num_features, \n",
    "                                gat_hidden_size, \n",
    "                                edge_dim=num_edge_features, \n",
    "                                dropout=0.2,\n",
    "                                heads=n_gat_heads)\n",
    "        self.pool_1 = TopKPooling(n_gat_heads * gat_hidden_size, ratio=0.75)\n",
    "        self.pair_norm_1 = PairNorm(n_gat_heads * gat_hidden_size)\n",
    "                \n",
    "        self.conv_2 = GATv2Conv(n_gat_heads * gat_hidden_size, \n",
    "                                gat_hidden_size, \n",
    "                                edge_dim=num_edge_features,\n",
    "                                dropout=0.2,\n",
    "                                heads=n_gat_heads)        \n",
    "        self.pool_2 = TopKPooling(n_gat_heads * gat_hidden_size, ratio=0.75)\n",
    "        self.pair_norm_2 = PairNorm(n_gat_heads * gat_hidden_size)\n",
    "        \n",
    "        self.conv_3 = GATv2Conv(n_gat_heads * gat_hidden_size, \n",
    "                                gat_hidden_size, \n",
    "                                edge_dim=num_edge_features,\n",
    "                                dropout=0.2,\n",
    "                                heads=n_gat_heads)        \n",
    "        self.pool_3 = TopKPooling(n_gat_heads * gat_hidden_size, ratio=0.75)\n",
    "        self.pair_norm_3 = PairNorm(n_gat_heads * gat_hidden_size)\n",
    "        \n",
    "        ### Estimator block\n",
    "        self.dropout_desc = Dropout(p=0.2)\n",
    "        self.dropout_dense = Dropout(p=0.2)\n",
    "        \n",
    "        descs_comp=0\n",
    "        self.batch_norm_1 = BatchNorm1d(2 * n_gat_heads * gat_hidden_size + descs_comp)        \n",
    "        self.hidden_dense = Linear(2 * n_gat_heads * gat_hidden_size + descs_comp, dense_hidden_size)\n",
    "        self.batch_norm_2 = BatchNorm1d(dense_hidden_size)\n",
    "        self.out = Linear(dense_hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch_index, desc_pca):\n",
    "        \n",
    "        ### Graph embedding block\n",
    "        nn = F.relu(self.conv_1(x, edge_index, edge_attr))   \n",
    "        nn, edge_index, edge_attr, batch_index, _, _ = self.pool_1(nn, edge_index, edge_attr, batch_index)\n",
    "        nn = self.pair_norm_1(nn, batch_index)        \n",
    "        \n",
    "        nn = F.relu(self.conv_2(nn, edge_index, edge_attr))\n",
    "        nn, edge_index, edge_attr, batch_index, _, _ = self.pool_2(nn, edge_index, edge_attr, batch_index)\n",
    "        nn = self.pair_norm_2(nn, batch_index)\n",
    "        \n",
    "        nn = F.relu(self.conv_3(nn, edge_index, edge_attr))\n",
    "        nn, edge_index, edge_attr, batch_index, _, _ = self.pool_3(nn, edge_index, edge_attr, batch_index)\n",
    "        nn = self.pair_norm_3(nn, batch_index)\n",
    "                \n",
    "        embedding = torch.cat([global_mean_pool(nn, batch_index), \n",
    "                               global_max_pool(nn, batch_index)], dim=1)\n",
    "        \n",
    "        ### Estimator block\n",
    "        nn = embedding        \n",
    "        \n",
    "        #desc_pca = self.dropout_desc(desc_pca)\n",
    "        \n",
    "        #nn = torch.cat([nn, desc_pca], axis=-1)     \n",
    "        nn = self.batch_norm_1(nn)     \n",
    "\n",
    "        nn = self.hidden_dense(nn)\n",
    "        nn = self.dropout_dense(nn)\n",
    "        nn = self.batch_norm_2(nn)   \n",
    "        \n",
    "        nn = F.relu(nn)\n",
    "        \n",
    "        out = self.out(nn)\n",
    "        \n",
    "        return out, embedding        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two validation scenarios are performed:\n",
    " - optimistic scenario, in which train/validation/test splits have similarly distributed samples (IID)\n",
    " - realistic scenario, in which train/validation/test splits contain observations sampled from dissimilar distributions (ex. compounds have different scaffolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.model_eval import train, dro_train, predict, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(dataset)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_loop(train_loader, val_loader, test_loader,\n",
    "                        epochs=2000, early_stop_patience=15,\n",
    "                        train_fcn=train):\n",
    "    \n",
    "    model = DopamineGAT(num_node_features, num_edge_features,\n",
    "                        gat_hidden_size=8,\n",
    "                        dense_hidden_size=32,\n",
    "                        n_gat_heads=5)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Keep model in training mode to use MC dropout confidence estimation\n",
    "    model.train() \n",
    "    \n",
    "    loss_f = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    stats = {'loss': [], 'q2_train': [], 'q2_val': [], 'q2_val_conf': []}\n",
    "\n",
    "    early_stop_scores = []\n",
    "    best_state = model.state_dict()\n",
    "    \n",
    "    pbar = tqdm(range(epochs))\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        loss = train_fcn(model, train_loader, loss_f, optimizer, epoch)\n",
    "        \n",
    "        # Validate the model and early stop if triggered\n",
    "        if epoch%10 == 0:\n",
    "            \n",
    "            q2_train, _ = metric(model, train_loader)\n",
    "            \n",
    "            q2_val, q2_val_confident = metric(model, val_loader)\n",
    "            \n",
    "            early_stop_scores.append(q2_val)   \n",
    "\n",
    "            if (np.argmax(np.flip(early_stop_scores)) > early_stop_patience):\n",
    "                break\n",
    "\n",
    "            if np.max(early_stop_scores) == early_stop_scores[-1]:\n",
    "                best_state = model.state_dict()\n",
    "\n",
    "            stats['loss'].append(loss.detach().numpy())\n",
    "            stats['q2_train'].append(q2_train)\n",
    "            stats['q2_val'].append(q2_val)\n",
    "            stats['q2_val_conf'].append(q2_val_confident)\n",
    "\n",
    "        report = (loss, q2_train, q2_val, q2_val_confident)\n",
    "        \n",
    "        pbar.set_description('Loss: %.3f | Train: %.3f  | Val: %.3f | Val(conf): %.3f'%report)\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    q2_test, q2_test_confident = metric(model, test_loader)\n",
    "    \n",
    "    return q2_test, q2_test_confident, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats):\n",
    "    df = pd.DataFrame(index=stats.keys(), data=stats.values())\n",
    "    df = df.loc[['q2_train', 'q2_val', 'q2_val_conf']].T   \n",
    "    df.plot(logy=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation with similarly distributed training/validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iid_test(train_fcn):\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # The folds are stratified by target values\n",
    "    quants = np.array(pd.qcut([x.y for x in dataset], 5, labels=range(5), retbins=False))\n",
    "    splits = sss.split(np.arange(len(dataset)), labels)\n",
    "    \n",
    "    q2_test_scores = []\n",
    "    q2_test_confident_scores = []\n",
    "    \n",
    "    for fold_id, (train_val_index, test_index) in enumerate(splits):\n",
    "\n",
    "        # Validation set for early stopping / hyperparameters optimization\n",
    "        train_val_data = dataset[train_val_index]\n",
    "\n",
    "        train_index, val_index = train_test_split(train_val_index, test_size=0.2,\n",
    "                                                  stratify=np.array([x.cluster for x in train_val_data]))    \n",
    "\n",
    "        train_loader = DataLoader(dataset[train_index], batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(dataset[val_index], batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[test_index], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        q2_test, q2_test_confident, stats = train_val_test_loop(train_loader, val_loader, test_loader,\n",
    "                                                                epochs=EPOCHS, early_stop_patience=15, \n",
    "                                                                train_fcn=train_fcn)\n",
    "\n",
    "        print('Fold %d | Test Q2: %.3f |  Test Q2 (1/4 most confident): %.3f'%(fold_id, \n",
    "                                                                               q2_test, q2_test_confident))\n",
    "        plot_stats(stats)\n",
    "        \n",
    "        q2_test_scores.append(q2_test)\n",
    "        q2_test_confident_scores.append(q2_test_confident_scores)\n",
    "    \n",
    "    return q2_test, q2_test_confident, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation with training/validation sets sampled from dissimilar distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ood_test(train_fcn):\n",
    "\n",
    "    clusters = pd.DataFrame(labels).value_counts().iloc[:5]\n",
    "\n",
    "    for fold_id, test_cluster in enumerate(np.concatenate(clusters.index)):\n",
    "\n",
    "        # Validation set for early stopping / hyperparameters optimization\n",
    "        train_val_data = [r for r in dataset if r.cluster!=test_cluster]\n",
    "        val_cluster = np.random.choice(np.array([x.cluster for x in train_val_data]))\n",
    "\n",
    "        train_data = [r for r in train_val_data if r.cluster!=val_cluster]\n",
    "        val_data = [r for r in train_val_data if r.cluster==val_cluster]    \n",
    "        test_data = [r for r in dataset if r.cluster==test_cluster]\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        q2_test, q2_test_confident, stats = train_val_test_loop(train_loader, val_loader, test_loader,\n",
    "                                                                epochs=EPOCHS, early_stop_patience=15, \n",
    "                                                                train_fcn=train_fcn)\n",
    "\n",
    "        print('Fold %d | Test Q2: %.3f |  Test Q2 (1/4 most confident): %.3f'%(fold_id, q2_test, \n",
    "                                                                               q2_test_confident))\n",
    "        plot_stats(stats)\n",
    "    \n",
    "    return q2_test, q2_test_confident, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IID observations / regular training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iid_test_reg_train_res = iid_test(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD observations / regular training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ood_test_reg_train_res = ood_test(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributionally robust optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD observations / DRO training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ood_test_dro_train_res = ood_test(dro_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A test of the model in the IID data regime: \n",
    " - model achieves Q2 0.55 (+/- 0.02) on the test set\n",
    " - validation score of the model is a good indicator of the expected testing score\n",
    "2. A test of the model in the OOD data regime:\n",
    " - model achieves Q2 0.03 (+/- 0.28) on the test set\n",
    " - validation score is not a reliable indicator of the expected test score\n",
    "3. A test of the model in OOD data regime with DRO training:\n",
    " - model achieves Q2 -0.17 (+/- 0.26)\n",
    " - validation score of the model is a better indicator of the expected testing score than in regular training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data domain for both development and production datasets is a necessary condition for reliable model execution.\\\n",
    "The usefulness of training by distributionally robust optimization in the OOD data regime is limited to a better indication of expected statistical characteristics in production. Possibly, better performance of the DRO approach may be achieved with more expressive model archotectures or longer training times.\\\n",
    "For a model trained and tested in the IID data regime, confidence prediction with the Monte Carlo Dropout approach allows to select more reliable prediction and achieve better statistical characteristics for the most confident predictions. Reliable confidence estimation may be important for reducing False Positive Rate in virtual screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
